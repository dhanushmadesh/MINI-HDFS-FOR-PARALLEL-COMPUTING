# Mini HDFS for Parallel Computing

A **Mini Distributed Analytics System** that simulates a simplified Hadoop-like architecture using a custom **Mini HDFS** storage layer and **parallel MapReduce-style computation** in Python.

This project demonstrates core concepts of **distributed storage, metadata management, and parallel processing**, built entirely from scratch for academic and learning purposes.

---

## ğŸ“Œ Key Features

- Custom **Mini HDFS** with:
  - Namenode for metadata management
  - Multiple data nodes for chunk storage
- **Line-safe file splitting** (no broken words or partial lines)
- Parallel data processing using **Python multiprocessing**
- MapReduce-style execution:
  - Map phase on distributed chunks
  - Reduce phase for global aggregation
- Interactive **CLI interface** with colored output
- Supports multiple analytics jobs:
  - Total Word Count
  - Global Most Frequent Word

---

## ğŸ—ï¸ System Architecture

User
â”‚
â”‚ (Upload File)
â–¼
MAIN.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚
â”‚ calls â”‚ manages
â–¼ â–¼
MINI_HDFS.py Namenode
â”‚ â”‚
â”‚ splits file â””â”€â”€ metadata.json
â–¼
Data Nodes (nodes/)
â”œâ”€â”€ node1/
â”œâ”€â”€ node2/
â””â”€â”€ node3/
â””â”€â”€ chunk files

yaml
Copy code

---

## ğŸ“‚ Project Structure

MINI-HDFS-FOR-PARALLEL-COMPUTING/
â”‚
â”œâ”€â”€ MAIN.py # CLI + Parallel MapReduce engine
â”œâ”€â”€ MINI_HDFS.py # Mini HDFS implementation
â”œâ”€â”€ namenode/
â”‚ â””â”€â”€ metadata.json # File & chunk metadata
â”œâ”€â”€ nodes/
â”‚ â”œâ”€â”€ node1/ # Chunk storage
â”‚ â”œâ”€â”€ node2/
â”‚ â””â”€â”€ node3/
â””â”€â”€ .gitignore

yaml
Copy code

---

## âš™ï¸ How It Works

### 1ï¸âƒ£ File Upload & Storage
- User uploads a text file
- File is **split line-by-line** into fixed chunks
- Each chunk is stored in a separate data node
- Namenode records metadata (file â†’ chunks â†’ nodes)

### 2ï¸âƒ£ Parallel Processing
- Each chunk is processed **independently** using multiprocessing
- Map functions operate on chunk content
- Reduce phase aggregates results into a global output

### 3ï¸âƒ£ Supported Jobs
| Job | Description |
|----|------------|
| Word Count | Computes total number of words across all chunks |
| Most Frequent Word | Finds globally most common word |

---

## â–¶ï¸ How to Run

### Prerequisites
- Python 3.8+
- Required library:
```bash
pip install colorama
Run the program
bash
Copy code
python MAIN.py
Menu Options
markdown
Copy code
1. Upload & Split File
2. View Metadata
3. Run Parallel Analysis
4. Exit
